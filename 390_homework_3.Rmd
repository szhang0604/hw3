---
title: "390 HW 3"
output: html_document
date: "2024-02-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

**1**
In this homework, we will discuss support vector machines and tree-based methods. I will begin by simulating some data for you to use with SVM.

```{r sim, echo=TRUE}
library(e1071)
set.seed(1) 
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
dat
plot(x, col=y)
```

**1.1**
Quite clearly, the above data is not linearly separable. Create a training-testing partition with 100 random observations in the training partition. Fit an svm on this training data using the radial kernel, and tuning parameters γ=1, cost =1. Plot the svm on the training data.

```{r split, echo=TRUE}
# training/testing split
train <- sample(200,100)
traindata <- dat[train, ]
testdata <- dat[-train, ]

# fitting svm to training data
svmfit = svm(y~., data = traindata, kernel = "radial", gamma = 1, cost = 1, scale = FALSE)
svmfit

# plotting fit 
plot(svmfit, traindata)

```

**1.2**
Notice that the above decision boundary is decidedly non-linear. It seems to perform reasonably well, but there are indeed some misclassifications. Let’s see if increasing the cost 1 helps our classification error rate. Refit the svm with the radial kernel, γ=1
, and a cost of 10000. Plot this svm on the training data.
```{r model, echo=TRUE}
svmfit2 = svm(y~., data = traindata, kernel = "radial", gamma = 1, cost = 10000, scale = FALSE)
svmfit2
plot(svmfit2, traindata)
```
**1.3**
It would appear that we are better capturing the training data, but comment on the dangers (if any exist), of such a model.

A danger of such a complex model would be overfitting, where the model fits the training data so closely that it cannot give accurate predictions for the test data. Another danger would be the loss of interpretability.

**1.4**
Create a confusion matrix by using this svm to predict on the current testing partition. Comment on the confusion matrix. Is there any disparity in our classification results?
```{r matrix, echo=TRUE}
table(true=dat[-train,"y"], pred=predict(svmfit2, newdata=dat[-train,]))
36/41
```
The confusion matrix reveals an 87.8% accuracy rate. We see a disparity in misclassifications between classes, namely a higher proportion of Class 2 observations misclassified than Class 1.

**1.5**
Is this disparity because of imbalance in the training/testing partition? Find the proportion of class 2 in your training partition and see if it is broadly representative of the underlying 25% of class 2 in the data as a whole.

```{r rep, echo=TRUE}
table(traindata$y)["2"]
```
Training partition is 24% class 2 vs. 25% of class 2 in the data as a whole, meaning the training set is broadly representative of class 2.

**1.6**
Let’s try and balance the above to solutions via cross-validation. Using the tune function, pass in the training data, and a list of the following cost and γ values: {0.1, 1, 10, 100, 1000} and {0.5, 1,2,3,4}. Save the output of this function in a variable called tune.out. I will take tune.out and use the best model according to error rate to test on our data. I will report a confusion matrix corresponding to the 100 predictions.
```{r cv, echo=TRUE}
table(traindata$y)["2"]
set.seed(1)
tune.out <- tune(svm, y ~ ., data = traindata, kernel = "radial", scale = TRUE,
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 1,2,3,4)))
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,])) # only 5% misclassified
39/41
```
**1.7**
Comment on the confusion matrix. How have we improved upon the model in question 2 and what qualifications are still necessary for this improved model.

The new model has a 95.1% accuracy rate, which is a 7.3 pp improvement from the first model. 

**2**
Let's turn now to decision trees.
```{r set, echo=TRUE}
library(kmed)
data(heart)
library(tree)
```
**2.1**
The response variable is currently a categorical variable with four levels. Convert heart disease into binary categorical variable. Then, ensure that it is properly stored as a factor.
```{r factor, echo=TRUE}
hdisease = as.factor(ifelse(heart$class == 0, "No", "Yes"))
heart <- data.frame(heart, hdisease)
heart <- heart[, -14]
```
**2.2**
Train a classification tree on a 240 observation training subset (using the seed I have set for you). Plot the tree.
```{r split2, echo=TRUE}
set.seed(101)
train <- sample(297,240)
traindata <- dat[train, ]
testdata <- dat[-train, ]
```
**2.3**
Use the trained model to classify the remaining testing points. Create a confusion matrix to evaluate performance. Report the classification error rate.
```{r tree, echo=TRUE}
# fitting tree on training data
tree.heart = tree(hdisease ~., heart, subset = train)
plot(tree.heart)
text(tree.heart, pretty=0)

# confusion matrix 
heart.pred = predict(tree.heart, heart[-train,], type="class")
with(heart[-train,], table(heart.pred, hdisease))
(28+18)/(28+18+8+3) # 80.7% accuracy
```
**2.4**
Above we have a fully grown (bushy) tree. Now, cross validate it using the cv.tree command. Specify cross validation to be done according to the misclassification rate. Choose an ideal number of splits, and plot this tree. Finally, use this pruned tree to test on the testing set. Report a confusion matrix and the misclassification rate.
```{r prune, echo=TRUE}
# cross validation
cv.heart <- cv.tree(tree.heart, FUN = prune.misclass)
cv.heart

plot(cv.heart$size, cv.heart$dev, type = "b")

set.seed(13)

# plotting pruned tree (2 terminal nodes)
prune.tree <- prune.misclass(tree.heart, best = 2)
plot(prune.tree)
text(prune.tree, pretty = 0)

# predicting on testing set 
tree.pred = predict(prune.tree, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, hdisease))
(28+12)/(28+12+9+8) # 70.18 accuracy

# plotting pruned tree (4 terminal nodes)
prune.tree <- prune.misclass(tree.heart, best = 4)
plot(prune.tree)
text(prune.tree, pretty = 0)

# predicting on testing set (size 4)
prune.tree <- prune.misclass(tree.heart, best = 4)
plot(prune.tree)
text(prune.tree, pretty = 0)
tree.pred = predict(prune.tree, heart[-train,], type="class")
with(heart[-train,], table(tree.pred, hdisease))
(26+17)/(26+17+4+10) # 75.438% accuracy - increase from size 2 tree
```
**2.5**
Discuss the trade-off in accuracy and interpretability in pruning the above tree.

We lose a non-trivial percentage of accuracy with a simpler tree. The full tree before pruning yields an accuracy rate of 80.7%, while the pruned tree with 4 terminal nodes has an accuracy rae of 75.438%. The pruned tree with only 2 terminal nodes has an accuracy rate of 70.18%, a 10% decrease from the full tree. 

**2.6**
Discuss the ways a decision tree could manifest algorithmic bias.

Overfitting with an overly complex decision tree and biased training data could manifest algorithmic bias. 
